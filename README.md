# [使用Keras中LeNet模型对Mnist手写数字进行识别](https://github.com/Arieswk/DL_LeNet_Mnist/blob/master/LeNet_Mnist.py)  
# 知识点  
##### 深度学习会自己找到数据的特征规律，而传统机器学习往往需要专家来告诉机器采用什么样的模型算法。  
* 节点：神经网络是由神经元组成的，也称之为节点，它们分布在神经网络的各个层中，这些层包括输入层，输出层和隐藏层。  
* 输入层：负责接收信号，并分发到隐藏层。一般我们将数据传给输入层。
* 输出层：负责输出计算结果，一般来说输出层节点数等于我们要分类的个数。
* 隐藏层：除了输入层和输出层外的神经网络都属于隐藏层，隐藏层可以是一层也可以是多层，每个隐藏层都会把前一层节点传输出来的数据进行计算，相当于把数据抽象到另一个维度的空间中，可以更好地提取和计算数据的特征。  
* 工作原理：神经网络就好比一个黑盒子，我们只需要告诉这个黑盒子输入数据和输出数据，神经网络就可以自我训练。一旦训练好之后，就可以像黑盒子一样使用，当你传入一个新的数据时，它就会告诉你对应的输出结果。在训练过程中，神经网络主要是通过前向传播和反向传播机制运作的。  
* 前向传播：数据从输入层传递到输出层的过程叫做前向传播。这个过程的计算结果通常是通过上一层的神经元的输出经过矩阵运算和激活函数得到的。这样就完成了每层之间的神经元数据的传输。  
* 反向传播：当前向传播作用到输出层得到分类结果之后，我们需要与实际值进行比对，从而得到误差。反向传播也叫作误差反向传播，核心原理是通过代价函数对网络中的参数进行修正，这样更容易让网络参数得到收敛。  
##### 常用的三种神经网络  
* FNN（Fully-connected Neural Network）指的是全连接神经网络，全连接的意思是每一层的神经元与上一层的所有神经元都是连接的。不过在实际使用中，全连接的参数会过多，导致计算量过大。因此在实际使用中全连接神经网络的层数一般比较少。  
* CNN叫作卷积神经网络，在图像处理中有广泛的应用，了解图像识别的同学对这个词一定不陌生。CNN 网络中，包括了卷积层、池化层和全连接层。  
  * 卷积层相当于一个滤镜的作用，它可以把图像进行分块，对每一块的图像进行变换操作。  
  * 池化层相当于对神经元的数据进行降维处理，这样输出的维数就会减少很多，从而降低整体的计算量。  
* RNN称为循环神经网络，它的特点是神经元的输出可以在下一个时刻作用到自身，这样 RNN 就可以看做是在时间上传递的神经网络。它可以应用在语音识别、自然语言处理等与上下文相关的场景。  
 ##### 常用的三种激活函数及作用
 * Sigmoid、hanh、ReLU  
 * 这些激活函数通常都是非线性的函数，使用它们的目的是把线性数值映射到非线性空间中。卷积操作实际上是两个矩阵之间的乘法，得到的结果也是线性的。只有经过非线性的激活函数运算之后，才能映射到非线性空间中，这样也可以让神经网络的表达能力更强大。  
# 总结
一个基于 CNN 的深度学习网络通常是几组卷积层之后，再连接多个全连接层，最后再接 Output 全连接层，而每组的卷积层都是" 卷积层+ -> 池化层? "的结构。  
其中，其"+"代表 1 个或多个,"?"代表 0 个或 1 个。
